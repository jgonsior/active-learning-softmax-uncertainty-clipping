{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jg/.local/share/virtualenvs/active-learning-softmax-uncertainty-clippi-y7QO68ki/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ast import FormattedValue\n",
    "from collections import OrderedDict, defaultdict\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "from dis import findlinestarts\n",
    "import enum\n",
    "from heapq import nlargest\n",
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "from turtle import title\n",
    "from typing import Any, Dict, Tuple\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import numpy as np\n",
    "from regex import D\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "from run_experiment import (\n",
    "    full_param_grid,\n",
    "    dev_param_grid,\n",
    "    baselines_param_grid,\n",
    "    my_methods_param_grid,\n",
    "    generate_workload,\n",
    ")\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from dataset_loader import load_my_dataset\n",
    "from small_text import data\n",
    "\n",
    "font_size = 5.8\n",
    "\n",
    "tex_fonts = {\n",
    "    # Use LaTeX to write all text\n",
    "    #\"text.usetex\": True,\n",
    "    # \"text.usetex\": False,\n",
    "    #\"font.family\": \"times\",\n",
    "    # Use 10pt font in plots, to match 10pt font in document\n",
    "    \"axes.labelsize\": font_size,\n",
    "    \"font.size\": font_size,\n",
    "    # Make the legend/label fonts a little smaller\n",
    "    \"legend.fontsize\": font_size,\n",
    "    \"xtick.labelsize\": font_size,\n",
    "    \"ytick.labelsize\": font_size,\n",
    "    \"xtick.bottom\": True,\n",
    "    # \"figure.autolayout\": True,\n",
    "}\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"paper\")\n",
    "plt.rcParams.update(tex_fonts)  # type: ignore\n",
    "\n",
    "\n",
    "# https://jwalton.info/Embed-Publication-Matplotlib-Latex/\n",
    "def set_matplotlib_size(width, fraction=1):\n",
    "    \"\"\"Set figure dimensions to avoid scaling in LaTeX.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    width: float\n",
    "            Document textwidth or columnwidth in pts\n",
    "    fraction: float, optional\n",
    "            Fraction of the width which you wish the figure to occupy\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig_dim: tuple\n",
    "            Dimensions of figure in inches\n",
    "    \"\"\"\n",
    "    # Width of figure (in pts)\n",
    "    fig_width_pt = width * fraction\n",
    "\n",
    "    # Convert from pt to inches\n",
    "    inches_per_pt = 1 / 72.27\n",
    "\n",
    "    # Golden ratio to set aesthetic figure height\n",
    "    # https://disq.us/p/2940ij3\n",
    "    golden_ratio = (5**0.5 - 1) / 2\n",
    "\n",
    "    # Figure width in inches\n",
    "    fig_width_in = fig_width_pt * inches_per_pt\n",
    "    # Figure height in inches\n",
    "    fig_height_in = fig_width_in * golden_ratio\n",
    "\n",
    "    fig_dim = (fig_width_in, fig_height_in)\n",
    "\n",
    "    return fig_dim\n",
    "\n",
    "\n",
    "width = 505.89\n",
    "width = 358.5049\n",
    "\n",
    "\n",
    "def queried_samples_table(\n",
    "    exp_name: str,\n",
    "    transformer_model_name: str,\n",
    "    dataset: str,\n",
    "    initially_labeled_samples: int,\n",
    "    batch_size: int,\n",
    "    param_grid: Dict[str, Any],\n",
    "    num_iterations: int,\n",
    "    metric,\n",
    "    table_title_prefix: str,\n",
    "):\n",
    "    # available metrics: train_accs, test_acc, train_eces, test_ece, y_probas_train/test, times_elapsed, times_elapsed_model, queried_indices, acc_bins_train, proba_+ins, confidence scores\n",
    "    print(f\"Metric: {metric}\")\n",
    "    grouped_data = _load_grouped_data(\n",
    "        exp_name,\n",
    "        transformer_model_name,\n",
    "        dataset,\n",
    "        initially_labeled_samples,\n",
    "        batch_size,\n",
    "        param_grid,\n",
    "        num_iterations,\n",
    "        metric,\n",
    "    )\n",
    "    table_data = []\n",
    "    for (strat_a, strat_b) in itertools.combinations(grouped_data.keys(), 2):\n",
    "        print(f\"{strat_a} vs {strat_b}\")\n",
    "        jaccards = []\n",
    "        for random_seed_data_a, random_seed_data_b in zip(\n",
    "            grouped_data[strat_a], grouped_data[strat_b]\n",
    "        ):\n",
    "            queried_a = np.array(random_seed_data_a).flatten()\n",
    "            queried_b = np.array(random_seed_data_b).flatten()\n",
    "            jaccard = len(np.intersect1d(queried_a, queried_b)) / len(\n",
    "                np.union1d(queried_a, queried_b)\n",
    "            )\n",
    "            jaccards.append(jaccard)\n",
    "        table_data.append(\n",
    "            (f\"{strat_a} vs {strat_b}\", jaccards, np.mean(jaccards), np.std(jaccards))\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(table_data, columns=[\"Strategy\", \"Jaccard\", \"Mean\", \"Std\"])\n",
    "    df.sort_values(by=\"Mean\", inplace=True)\n",
    "    print(tabulate(df, headers=\"keys\"))\n",
    "\n",
    "    table_file = Path(\n",
    "        f\"tables/queried_samples_data_{table_title_prefix}-{metric}_{exp_name}_{transformer_model_name}_{dataset}_{initially_labeled_samples}_{batch_size}_{num_iterations}.tex\"\n",
    "    )\n",
    "    table_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    table_file.write_text(tabulate(df, headers=\"keys\", tablefmt=\"latex_booktabs\"))\n",
    "\n",
    "\n",
    "def runtime_plots(\n",
    "    exp_name: str,\n",
    "    transformer_model_name: str,\n",
    "    dataset: str,\n",
    "    initially_labeled_samples: int,\n",
    "    batch_size: int,\n",
    "    param_grid: Dict[str, Any],\n",
    "    num_iterations: int,\n",
    "    metric,\n",
    "    table_title_prefix,\n",
    "):\n",
    "    # available metrics: train_accs, test_acc, train_eces, test_ece, y_probas_train/test, times_elapsed, times_elapsed_model, queried_indices, acc_bins_train, proba_+ins, confidence scores\n",
    "    print(f\"Metric: {metric}\")\n",
    "    grouped_data = _load_grouped_data(\n",
    "        exp_name,\n",
    "        transformer_model_name,\n",
    "        dataset,\n",
    "        initially_labeled_samples,\n",
    "        batch_size,\n",
    "        param_grid,\n",
    "        num_iterations,\n",
    "        metric,\n",
    "    )\n",
    "    if len(grouped_data) == 0:\n",
    "        return\n",
    "\n",
    "    # sum up elapsed times\n",
    "    df_data = []\n",
    "    for k, v in grouped_data.items():\n",
    "        for value in v:\n",
    "            df_data.append((k, sum(value)))\n",
    "    data_df = pd.DataFrame(df_data, columns=[\"Strategy\", metric])\n",
    "    print(data_df)\n",
    "    fig = plt.figure(figsize=set_matplotlib_size(width, fraction=1.0))\n",
    "    sns.catplot(data=data_df, y=\"Strategy\", x=metric, kind=\"bar\")\n",
    "\n",
    "    plots_path = Path(\"plots/\")\n",
    "    plots_path.mkdir(exist_ok=True)\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"plots/{metric}_{exp_name}_{transformer_model_name}_{dataset}_{initially_labeled_samples}_{batch_size}_{num_iterations}.jpg\",\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "    )\n",
    "    plt.savefig(\n",
    "        f\"plots/{metric}_{exp_name}_{transformer_model_name}_{dataset}_{initially_labeled_samples}_{batch_size}_{num_iterations}.pdf\",\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "        pad_inches=0,\n",
    "    )\n",
    "    plt.clf()\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    table_data = []\n",
    "\n",
    "    for k, v in grouped_data.items():\n",
    "        v = [sum(value) for value in v]\n",
    "        table_data.append((k, v, np.mean(v), np.std(v)))\n",
    "    df = pd.DataFrame(table_data, columns=[\"Strategy\", \"Values\", \"Mean\", \"Std\"])\n",
    "    df.sort_values(by=\"Mean\", inplace=True)\n",
    "    print(tabulate(df, headers=\"keys\"))\n",
    "\n",
    "    table_file = Path(\n",
    "        f\"tables/runtime_{table_title_prefix}-{metric}_{exp_name}_{transformer_model_name}_{dataset}_{initially_labeled_samples}_{batch_size}_{num_iterations}.tex\"\n",
    "    )\n",
    "    table_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    table_file.write_text(tabulate(df, headers=\"keys\", tablefmt=\"latex_booktabs\"))\n",
    "\n",
    "\n",
    "def uncertainty_histogram_plots(\n",
    "    exp_name: str,\n",
    "    transformer_model_name: str,\n",
    "    dataset: str,\n",
    "    initially_labeled_samples: int,\n",
    "    batch_size: int,\n",
    "    param_grid: Dict[str, Any],\n",
    "    num_iterations: int,\n",
    "    metric,\n",
    "    table_title_prefix: str,\n",
    "):\n",
    "    # available metrics: train_accs, test_acc, train_eces, test_ece, y_probas_train/test, times_elapsed, times_elapsed_model, queried_indices, acc_bins_train, proba_+ins, confidence scores\n",
    "    print(f\"Metric: {metric}\")\n",
    "    grouped_data = _load_grouped_data(\n",
    "        exp_name,\n",
    "        transformer_model_name,\n",
    "        dataset,\n",
    "        initially_labeled_samples,\n",
    "        batch_size,\n",
    "        param_grid,\n",
    "        num_iterations,\n",
    "        metric,\n",
    "    )\n",
    "    if len(grouped_data) == 0:\n",
    "        return\n",
    "    # print(grouped_data)\n",
    "    df_data = []\n",
    "    for k, v in grouped_data.items():\n",
    "        for random_seed in v:\n",
    "            # print(random_seed)\n",
    "            for i, iteration in enumerate(random_seed):\n",
    "                for v in iteration:\n",
    "                    if metric != \"confidence_scores\":\n",
    "                        v = np.max(v)\n",
    "                    if v < 0:\n",
    "                        v = v * (-1)\n",
    "                    df_data.append((k, v, i))\n",
    "    df = pd.DataFrame(data=df_data, columns=[\"Strategy\", metric, \"iteration\"])\n",
    "\n",
    "    \"\"\"sns.displot(\n",
    "        data=df,\n",
    "        x=metric,\n",
    "        col=\"Strategy\",\n",
    "        row=\"iteration\",\n",
    "        # binwidth=3,\n",
    "        # height=3,\n",
    "        facet_kws=dict(margin_titles=True),\n",
    "    )\n",
    "    plt.savefig(\n",
    "        f\"plots/{metric}_{exp_name}_{transformer_model_name}_{dataset}_{initially_labeled_samples}_{batch_size}_{num_iterations}_grouped.jpg\"\n",
    "    )\n",
    "    plt.clf()\n",
    "    plt.close('all')\n",
    "    \"\"\"\n",
    "\n",
    "    for strat in df[\"Strategy\"].unique():\n",
    "        mv = df.loc[df[\"Strategy\"] == strat][metric].astype(np.float16)\n",
    "        if np.nanmax(mv) == np.inf:\n",
    "            max_value = np.iinfo(np.int16).max\n",
    "        else:\n",
    "            max_value = np.nanmax(mv)\n",
    "        if np.nanmin(mv) == 0 and max_value == 0:\n",
    "            continue\n",
    "        counts, bins = np.histogram(mv, bins=100, range=(np.nanmin(mv), max_value))\n",
    "\n",
    "        fig = plt.figure(figsize=set_matplotlib_size(width, fraction=0.5))\n",
    "        plt.hist(\n",
    "            bins[:-1],\n",
    "            weights=counts,\n",
    "            bins=bins,\n",
    "        )\n",
    "        plt.title(f\"{strat}\")\n",
    "        plot_path = Path(\n",
    "            f\"./plots/{table_title_prefix}-{metric}_{exp_name}_{transformer_model_name}_{dataset}_{initially_labeled_samples}_{batch_size}_{num_iterations}\"\n",
    "        )\n",
    "        plot_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        plt.savefig(\n",
    "            plot_path / f\"{strat.replace('/', '-')}.jpg\",\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0,\n",
    "        )\n",
    "        plt.savefig(\n",
    "            plot_path / f\"{strat.replace('/', '-')}.pdf\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0,\n",
    "        )\n",
    "        plt.clf()\n",
    "        plt.close(\"all\")\n",
    "\n",
    "        for iteration in df[\"iteration\"].unique():\n",
    "            mv = df.loc[(df[\"Strategy\"] == strat) & (df[\"iteration\"] == iteration)][\n",
    "                metric\n",
    "            ].astype(np.float16)\n",
    "            if np.nanmax(mv) == np.inf:\n",
    "                max_value = np.iinfo(np.int16).max\n",
    "            else:\n",
    "                max_value = np.nanmax(mv)\n",
    "            if np.nanmin(mv) == 0 and max_value == 0:\n",
    "                continue\n",
    "            counts, bins = np.histogram(mv, bins=100, range=(np.nanmin(mv), max_value))\n",
    "\n",
    "            fig = plt.figure(figsize=set_matplotlib_size(width, fraction=0.5))\n",
    "            plt.hist(\n",
    "                bins[:-1],\n",
    "                weights=counts,\n",
    "                bins=bins,\n",
    "            )\n",
    "\n",
    "            #  fig = plt.figure(figsize=set_matplotlib_size(width, fraction=1.0))\n",
    "            #  sns.histplot(\n",
    "            #  data=df.loc[(df[\"Strategy\"] == strat) & (df[\"iteration\"] == iteration)],\n",
    "            #  x=metric,\n",
    "            #  )\n",
    "            plt.title(f\"{strat}: {iteration}\")\n",
    "            plot_path = Path(\n",
    "                f\"./plots/{table_title_prefix}-{metric}_{exp_name}_{transformer_model_name}_{dataset}_{initially_labeled_samples}_{batch_size}_{num_iterations}/{strat.replace('/', '-')}/\"\n",
    "            )\n",
    "            plot_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "            plt.savefig(\n",
    "                plot_path / f\"{iteration}.jpg\", bbox_inches=\"tight\", pad_inches=0\n",
    "            )\n",
    "            plt.savefig(\n",
    "                plot_path / f\"{iteration}.pdf\",\n",
    "                dpi=300,\n",
    "                bbox_inches=\"tight\",\n",
    "                pad_inches=0,\n",
    "            )\n",
    "            plt.clf()\n",
    "            plt.close(\"all\")\n",
    "\n",
    "\n",
    "def _convert_config_to_path(config_dict) -> Path:\n",
    "    params = OrderedDict(sorted(config_dict.items(), key=lambda t: t[0]))\n",
    "\n",
    "    exp_results_dir = Path(\n",
    "        # \"exp_results_taurus_with_class_weights/\"\n",
    "        \"exp_results/\"\n",
    "        + \"-\".join([str(a) for a in params.values()])\n",
    "    )\n",
    "    return exp_results_dir\n",
    "\n",
    "\n",
    "def _load_grouped_data(\n",
    "    exp_name: str,\n",
    "    transformer_model_name: str,\n",
    "    dataset: str,\n",
    "    initially_labeled_samples: int,\n",
    "    batch_size: int,\n",
    "    param_grid: Dict[str, Any],\n",
    "    num_iterations: int,\n",
    "    metric=\"test_acc\",\n",
    "    ignore_clipping_for_random_and_passive=True,\n",
    "):\n",
    "    grouped_data = {}\n",
    "    for query_strategy in param_grid[\"query_strategy\"]:\n",
    "        for uncertainty_method in param_grid[\"uncertainty_method\"]:\n",
    "            for lower_is_better in param_grid[\"lower_is_better\"]:\n",
    "                for uncertainty_clipping in param_grid[\"uncertainty_clipping\"]:\n",
    "                    if (\n",
    "                        query_strategy in [\"passive\", \"Rand\"]\n",
    "                        and uncertainty_clipping != 1.0\n",
    "                        and ignore_clipping_for_random_and_passive\n",
    "                    ):\n",
    "                        continue\n",
    "                    elif (\n",
    "                        query_strategy in [\"passive\", \"Rand\"]\n",
    "                        and uncertainty_clipping != 1.0\n",
    "                        and not ignore_clipping_for_random_and_passive\n",
    "                    ):\n",
    "                        uncertainty_clipping = 1.0\n",
    "                    key = f\"{query_strategy} ({uncertainty_method}) {lower_is_better}/{uncertainty_clipping}\"\n",
    "                    grouped_data[key] = []\n",
    "                    for random_seed in param_grid[\"random_seed\"]:\n",
    "                        # check if this configuration is available\n",
    "                        exp_results_dir = _convert_config_to_path(\n",
    "                            {\n",
    "                                \"uncertainty_method\": uncertainty_method,\n",
    "                                \"query_strategy\": query_strategy,\n",
    "                                \"exp_name\": exp_name,\n",
    "                                \"transformer_model_name\": transformer_model_name,\n",
    "                                \"dataset\": dataset,\n",
    "                                \"initially_labeled_samples\": initially_labeled_samples,\n",
    "                                \"random_seed\": random_seed,\n",
    "                                \"batch_size\": batch_size,\n",
    "                                \"num_iterations\": num_iterations,\n",
    "                                \"uncertainty_clipping\": uncertainty_clipping,\n",
    "                                \"lower_is_better\": lower_is_better,\n",
    "                            }\n",
    "                        )\n",
    "                        if exp_results_dir.exists():\n",
    "                            metrics = np.load(\n",
    "                                exp_results_dir / \"metrics.npz\", allow_pickle=True\n",
    "                            )\n",
    "                            # print(metrics.files)\n",
    "                            # args = json.loads(\n",
    "                            #    Path(exp_results_dir / \"args.json\").read_text()\n",
    "                            # )\n",
    "                            metric_values = metrics[metric].tolist()\n",
    "                            grouped_data[key].append(metric_values)\n",
    "\n",
    "                    if len(grouped_data[key]) == 0:\n",
    "                        del grouped_data[key]\n",
    "    return grouped_data\n",
    "\n",
    "\n",
    "def table_stats(\n",
    "    exp_name: str,\n",
    "    transformer_model_name: str,\n",
    "    dataset: str,\n",
    "    initially_labeled_samples: int,\n",
    "    batch_size: int,\n",
    "    param_grid: Dict[str, Any],\n",
    "    num_iterations: int,\n",
    "    metric=\"test_acc\",\n",
    "    table_title_prefix=\"\",\n",
    "    consider_last_n=21,\n",
    "):\n",
    "    # available metrics: train_accs, test_acc, train_eces, test_ece, y_probas_train/test, times_elapsed, times_elapsed_model, queried_indices, acc_bins_train, proba_+ins, confidence scores\n",
    "    print(f\"Metric: {metric}\")\n",
    "    grouped_data = _load_grouped_data(\n",
    "        exp_name,\n",
    "        transformer_model_name,\n",
    "        dataset,\n",
    "        initially_labeled_samples,\n",
    "        batch_size,\n",
    "        param_grid,\n",
    "        num_iterations,\n",
    "        metric,\n",
    "    )\n",
    "\n",
    "    def _learning_curves_plot(data):\n",
    "        df_data = []\n",
    "        for k, v in grouped_data.items():\n",
    "            for i, value in enumerate(v):\n",
    "                for j, val in enumerate(value):\n",
    "                    df_data.append((k, val, i, j))\n",
    "\n",
    "        data_df = pd.DataFrame(\n",
    "            df_data, columns=[\"Strategy\", metric, \"Random Seed\", \"Iteration\"]\n",
    "        )\n",
    "        fig = plt.figure(figsize=set_matplotlib_size(width, fraction=1.0))\n",
    "        sns.lineplot(x=\"Iteration\", y=metric, hue=\"Strategy\", data=data_df)\n",
    "\n",
    "        plots_path = Path(\"plots/\")\n",
    "        plots_path.mkdir(exist_ok=True)\n",
    "\n",
    "        plt.savefig(\n",
    "            f\"plots/{metric}_{exp_name}_{transformer_model_name}_{dataset}_{initially_labeled_samples}_{batch_size}_{num_iterations}.jpg\",\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0,\n",
    "        )\n",
    "        plt.savefig(\n",
    "            f\"plots/{metric}_{exp_name}_{transformer_model_name}_{dataset}_{initially_labeled_samples}_{batch_size}_{num_iterations}.pdf\",\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0,\n",
    "        )\n",
    "        plt.clf()\n",
    "        plt.close(\"all\")\n",
    "\n",
    "    _learning_curves_plot(grouped_data)\n",
    "\n",
    "    table_data = []\n",
    "\n",
    "    for k, v in grouped_data.items():\n",
    "        v = [x[-consider_last_n:] for x in v]\n",
    "        table_data.append((k, v, np.mean(v), np.std(v)))\n",
    "\n",
    "    df = pd.DataFrame(table_data, columns=[\"Strategy\", \"Values\", \"Mean\", \"Std\"])\n",
    "    df[\"Values\"] = df[\"Values\"].apply(lambda x: [sum(v) / len(v) for v in x])\n",
    "    df.sort_values(by=\"Mean\", inplace=True)\n",
    "    print(tabulate(df, headers=\"keys\"))\n",
    "\n",
    "    table_file = Path(\n",
    "        f\"tables/auc_stats_{table_title_prefix}-{metric}_{exp_name}_{transformer_model_name}_{dataset}_{initially_labeled_samples}_{batch_size}_{num_iterations}.tex\"\n",
    "    )\n",
    "    table_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    table_file.write_text(tabulate(df, headers=\"keys\", tablefmt=\"latex_booktabs\"))\n",
    "\n",
    "\n",
    "def display_run_experiment_stats(param_grid):\n",
    "    done_param_list, open_param_list, full_param_list = generate_workload(param_grid)\n",
    "    print(\"Open:\")\n",
    "    print(tabulate(open_param_list))\n",
    "\n",
    "    print()\n",
    "    print(\"Done:\")\n",
    "    done_param_list_without_folders = [params[0] for params in done_param_list]\n",
    "    print(tabulate(done_param_list_without_folders))\n",
    "\n",
    "    print()\n",
    "    print(\"full grid:\")\n",
    "    print(tabulate(param_grid, floatfmt=\".2f\", numalign=\"right\", headers=\"keys\"))\n",
    "\n",
    "\n",
    "# display_run_experiment_stats(param_grid=dev_param_grid)\n",
    "# display_run_experiment_stats(param_grid=my_methods_param_grid)\n",
    "# display_run_experiment_stats(param_grid=baselines_param_grid)\n",
    "\n",
    "\n",
    "def _filter_out_param(param_grid, param, values_to_delete):\n",
    "    for v in values_to_delete:\n",
    "        if v in param_grid[param]:\n",
    "            param_grid[param].remove(v)\n",
    "    return param_grid\n",
    "\n",
    "\n",
    "def _execute_parallel(param_grid, dataset: str):\n",
    "    for exp_name in param_grid[\"exp_name\"]:\n",
    "        for transformer_model_name in param_grid[\"transformer_model_name\"]:\n",
    "            for initially_labeled_samples in param_grid[\"initially_labeled_samples\"]:\n",
    "                for batch_size in param_grid[\"batch_size\"]:\n",
    "                    for num_iteration in param_grid[\"num_iterations\"]:\n",
    "                        print(\n",
    "                            f\"{exp_name} - {transformer_model_name} - {dataset} - {initially_labeled_samples} - {batch_size} - {num_iteration}\"\n",
    "                        )\n",
    "\n",
    "                        def _with_without_clipping(pg, clipping=True):\n",
    "                            if clipping:\n",
    "                                table_title_prefix = \"\"\n",
    "                                param_grid_new = _filter_out_param(\n",
    "                                    pg, \"uncertainty_clipping\", [0.95, 0.9, 0.7]\n",
    "                                )\n",
    "                            else:\n",
    "                                table_title_prefix = \"clipped\"\n",
    "                                param_grid_new = _filter_out_param(pg, \"\", [])\n",
    "\n",
    "                            table_stats(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid_new,\n",
    "                                num_iteration,\n",
    "                                metric=\"test_acc\",\n",
    "                                table_title_prefix=table_title_prefix + \"_last1\",\n",
    "                                consider_last_n=1,\n",
    "                            )\n",
    "                            table_stats(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid_new,\n",
    "                                num_iteration,\n",
    "                                metric=\"test_acc\",\n",
    "                                table_title_prefix=table_title_prefix + \"_last5\",\n",
    "                                consider_last_n=5,\n",
    "                            )\n",
    "\n",
    "                            table_stats(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid_new,\n",
    "                                num_iteration,\n",
    "                                metric=\"test_acc\",\n",
    "                                table_title_prefix=table_title_prefix,\n",
    "                                consider_last_n=21,\n",
    "                            )\n",
    "\n",
    "                            \"\"\"table_stats(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid_new,\n",
    "                                num_iteration,\n",
    "                                metric=\"test_ece\",\n",
    "                                table_title_prefix=table_title_prefix + \"_last5\",\n",
    "                                consider_last_n=5,\n",
    "                            )\n",
    "\n",
    "                            table_stats(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid_new,\n",
    "                                num_iteration,\n",
    "                                metric=\"test_ece\",\n",
    "                                table_title_prefix=table_title_prefix,\n",
    "                                consider_last_n=21,\n",
    "                            )\n",
    "\n",
    "                            runtime_plots(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid_new,\n",
    "                                num_iteration,\n",
    "                                metric=\"times_elapsed\",\n",
    "                                table_title_prefix=table_title_prefix,\n",
    "                            )\n",
    "\n",
    "                            queried_samples_table(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid_new,\n",
    "                                num_iteration,\n",
    "                                metric=\"queried_indices\",\n",
    "                                table_title_prefix=table_title_prefix,\n",
    "                            )\"\"\"\n",
    "\n",
    "                            \"\"\"uncertainty_histogram_plots(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid_new,\n",
    "                                num_iteration,\n",
    "                                metric=\"confidence_scores\",\n",
    "                                table_title_prefix=table_title_prefix,\n",
    "                            )\n",
    "\n",
    "                            uncertainty_histogram_plots(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid_new,\n",
    "                                num_iteration,\n",
    "                                metric=\"y_proba_test_active\",\n",
    "                                table_title_prefix=table_title_prefix,\n",
    "                            )\"\"\"\n",
    "\n",
    "                            \"\"\"runtime_plots(\n",
    "                                            exp_name,\n",
    "                                            transformer_model_name,\n",
    "                                            dataset,\n",
    "                                            initially_labeled_samples,\n",
    "                                            batch_size,\n",
    "                                            param_grid_new,\n",
    "                                            num_iteration,\n",
    "                                            metric=\"times_elapsed_model\",\n",
    "                                        )\"\"\"\n",
    "                            print()\n",
    "\n",
    "                        _with_without_clipping(copy.deepcopy(param_grid), clipping=True)\n",
    "                        _with_without_clipping(\n",
    "                            copy.deepcopy(param_grid), clipping=False\n",
    "                        )\n",
    "\n",
    "\n",
    "def tables_plots(param_grid):\n",
    "    for dataset in param_grid[\"dataset\"]:\n",
    "        _execute_parallel(param_grid, dataset)\n",
    "    # with parallel_backend(\"loky\", n_jobs=20):\n",
    "    #    Parallel()(\n",
    "    #        delayed(_execute_parallel)(param_grid, dataset)\n",
    "    #        for dataset in param_grid[\"dataset\"]\n",
    "    #    )\n",
    "\n",
    "\n",
    "def _rename_dataset_name(dataset):\n",
    "    if dataset == \"trec6\":\n",
    "        dataset2 = \"TR\"\n",
    "    elif dataset == \"ag_news\":\n",
    "        dataset2 = \"AG\"\n",
    "    elif dataset == \"subj\":\n",
    "        dataset2 = \"SU\"\n",
    "    elif dataset == \"rotten\":\n",
    "        dataset2 = \"RT\"\n",
    "    elif dataset == \"imdb\":\n",
    "        dataset2 = \"IM\"\n",
    "    elif dataset == \"sst2\":\n",
    "        dataset2 = \"S2\"\n",
    "    elif dataset == \"cola\":\n",
    "        dataset2 = \"CL\"\n",
    "    return dataset2\n",
    "\n",
    "\n",
    "def _rename_strat(strategy, clipping=True):\n",
    "    strategy = strategy.replace(\"1.0\", \"100\")\n",
    "    strategy = strategy.replace(\"0.95\", \"95\")\n",
    "    strategy = strategy.replace(\"0.9\", \"90\")\n",
    "    strategy = strategy.replace(\"trustscore (softmax)\", \"TrSc\")\n",
    "    # rename strategies\n",
    "    strategy = strategy.replace(\"True/\", \"\")\n",
    "    strategy = strategy.replace(\"MM (softmax)\", \"MM\")\n",
    "    strategy = strategy.replace(\"LC (softmax)\", \"LC\")\n",
    "    strategy = strategy.replace(\"LC (inhibited)\", \"IS\")\n",
    "    strategy = strategy.replace(\"LC (MonteCarlo)\", \"MoCa\")\n",
    "    strategy = strategy.replace(\"Ent (softmax)\", \"Ent\")\n",
    "    strategy = strategy.replace(\"Rand (softmax)\", \"Rand\")\n",
    "    strategy = strategy.replace(\"passive (softmax)\", \"Pass\")\n",
    "\n",
    "    strategy = strategy.replace(\"QBC_VE (softmax)\", \"VE\")\n",
    "    strategy = strategy.replace(\"QBC_KLD (softmax)\", \"KLD\")\n",
    "\n",
    "    strategy = strategy.replace(\"LC (evidential1)\", \"Evi\")\n",
    "\n",
    "    strategy = strategy.replace(\"LC (label_smoothing)\", \"LS\")\n",
    "    strategy = strategy.replace(\"LC (temp_scaling)\", \"TeSc\")\n",
    "\n",
    "    if not clipping:\n",
    "        strategy = strategy.replace(\" 100\", \"\")\n",
    "        strategy = strategy.replace(\" 95\", \"\")\n",
    "        strategy = strategy.replace(\" 90\", \"\")\n",
    "    else:\n",
    "        strategy = strategy.replace(\" 100\", \"\")\n",
    "\n",
    "    return strategy\n",
    "\n",
    "\n",
    "def full_boxplot(pg, clipping=True, metric=\"test_acc\", consider_last_n=21):\n",
    "    if clipping:\n",
    "        table_title_prefix = \"\"\n",
    "        param_grid = _filter_out_param(pg, \"uncertainty_clipping\", [0.95, 0.9, 0.7])\n",
    "    else:\n",
    "        table_title_prefix = \"clipped\"\n",
    "        param_grid = _filter_out_param(pg, \"\", [])\n",
    "\n",
    "    for exp_name in param_grid[\"exp_name\"]:\n",
    "        for transformer_model_name in param_grid[\"transformer_model_name\"]:\n",
    "            for initially_labeled_samples in param_grid[\"initially_labeled_samples\"]:\n",
    "                for batch_size in param_grid[\"batch_size\"]:\n",
    "                    for num_iteration in param_grid[\"num_iterations\"]:\n",
    "                        datasets = param_grid[\"dataset\"]\n",
    "                        table_file = Path(\n",
    "                            f\"final/merge_datasets_{metric}_{table_title_prefix}_{exp_name}_{transformer_model_name}_{consider_last_n}_{initially_labeled_samples}_{batch_size}_{num_iteration}.tex\"\n",
    "                        )\n",
    "                        plot_file = Path(\n",
    "                            f\"final/boxplots_{metric}_{table_title_prefix}_{exp_name}_{transformer_model_name}_{consider_last_n}_{initially_labeled_samples}_{batch_size}_{num_iteration}.pdf\"\n",
    "                        )\n",
    "                        print(table_file)\n",
    "\n",
    "                        groups = []\n",
    "                        for dataset in datasets:\n",
    "                            grouped_data = _load_grouped_data(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid,\n",
    "                                num_iteration,\n",
    "                                metric,\n",
    "                            )\n",
    "                            groups.append((dataset, grouped_data))\n",
    "\n",
    "                        table_data = []\n",
    "\n",
    "                        for (dataset, group) in groups:\n",
    "                            dataset2 = _rename_dataset_name(dataset)\n",
    "\n",
    "                            for k, v in group.items():\n",
    "                                print(k)\n",
    "                                k = _rename_strat(k, clipping=clipping)\n",
    "                                if k == \"Passive\":\n",
    "                                    continue\n",
    "                                v = [x[-consider_last_n:] for x in v]\n",
    "                                v = np.mean(v, axis=1)\n",
    "                                formatted_value = np.mean(v) * 100\n",
    "                                table_data.append((k, formatted_value))\n",
    "\n",
    "                        df = pd.DataFrame(table_data, columns=[\"Method\", \"Acc\"])\n",
    "                        df2 = df.groupby([\"Method\"]).mean().sort_values(\"Acc\")\n",
    "\n",
    "                        fig = plt.figure(\n",
    "                            figsize=set_matplotlib_size(width, fraction=1.0)\n",
    "                        )\n",
    "                        ax = sns.boxplot(data=df, x=\"Acc\", y=\"Method\", order=df2.index)\n",
    "                        plt.xlabel(\"\")\n",
    "                        plt.ylabel(\"\")\n",
    "\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(\n",
    "                            plot_file, dpi=300, bbox_inches=\"tight\", pad_inches=0\n",
    "                        )\n",
    "                        # plt.show()\n",
    "                        plt.clf()\n",
    "                        plt.close(\"all\")\n",
    "\n",
    "\n",
    "\n",
    "def full_table_stat(pg, clipping=True, metric=\"test_acc\", consider_last_n=21):\n",
    "    if clipping:\n",
    "        table_title_prefix = \"\"\n",
    "        param_grid = _filter_out_param(pg, \"\", [])\n",
    "    else:\n",
    "        table_title_prefix = \"clipped\"\n",
    "        param_grid = _filter_out_param(pg, \"uncertainty_clipping\", [0.95, 0.9, 0.7])\n",
    "\n",
    "    for exp_name in param_grid[\"exp_name\"]:\n",
    "        for transformer_model_name in param_grid[\"transformer_model_name\"]:\n",
    "            for initially_labeled_samples in param_grid[\"initially_labeled_samples\"]:\n",
    "                for batch_size in param_grid[\"batch_size\"]:\n",
    "                    for num_iteration in param_grid[\"num_iterations\"]:\n",
    "                        datasets = param_grid[\"dataset\"]\n",
    "                        table_file = Path(\n",
    "                            f\"final/merge_datasets_{metric}_{table_title_prefix}_{exp_name}_{transformer_model_name}_{consider_last_n}_{initially_labeled_samples}_{batch_size}_{num_iteration}.tex\"\n",
    "                        )\n",
    "                        plot_file = Path(\n",
    "                            f\"final/merge_datasets_{metric}_{table_title_prefix}_{exp_name}_{transformer_model_name}_{consider_last_n}_{initially_labeled_samples}_{batch_size}_{num_iteration}.pdf\"\n",
    "                        )\n",
    "                        print(table_file)\n",
    "\n",
    "                        groups = []\n",
    "                        for dataset in datasets:\n",
    "                            grouped_data = _load_grouped_data(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid,\n",
    "                                num_iteration,\n",
    "                                metric,\n",
    "                            )\n",
    "                            groups.append((dataset, grouped_data))\n",
    "\n",
    "                        table_data = {}\n",
    "\n",
    "                        for (dataset, group) in groups:\n",
    "                            dataset2 = _rename_dataset_name(dataset)\n",
    "\n",
    "                            for k, v in group.items():\n",
    "                                v = [x[-consider_last_n:] for x in v]\n",
    "                                v = np.mean(v, axis=1)\n",
    "                                std_v = np.std(v) * 100\n",
    "                                mean_v = np.mean(v) * 100\n",
    "\n",
    "                                formatted_value = f\"{mean_v:0.2f} +- ({std_v:0.2f})\"\n",
    "\n",
    "                                if k not in table_data.keys():\n",
    "                                    table_data[k] = {dataset2: formatted_value}\n",
    "                                else:\n",
    "                                    table_data[k][dataset2] = formatted_value\n",
    "\n",
    "                        for k, v in table_data.items():\n",
    "                            table_data[k][\"Mean\"] = \"{0:.2f}\".format(\n",
    "                                np.mean([float(x[:5]) for x in v.values()])\n",
    "                            )\n",
    "                        df = pd.DataFrame(table_data)\n",
    "                        df = df.T\n",
    "                        df.reset_index(inplace=True)\n",
    "                        df = df.rename(columns={\"index\": \"Method\"})\n",
    "                        df.sort_values(by=\"Mean\", inplace=True)\n",
    "\n",
    "                        df[\"Method\"] = df[\"Method\"].apply(\n",
    "                            lambda x: _rename_strat(x, clipping=clipping)\n",
    "                        )\n",
    "\n",
    "                        # df = df.set_index(\"Method\")\n",
    "                        print(df)\n",
    "\n",
    "                        print(\n",
    "                            tabulate(\n",
    "                                df,\n",
    "                                headers=\"keys\",\n",
    "                                floatfmt=(\"0.2f\"),\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "                        table_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        table_file.write_text(\n",
    "                            tabulate(\n",
    "                                df,\n",
    "                                headers=\"keys\",\n",
    "                                tablefmt=\"latex_booktabs\",\n",
    "                                showindex=False,\n",
    "                                floatfmt=(\"0.2f\"),\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "\n",
    "def show_values_on_bars(axs, h_v=\"v\", space=4.0, xlim_additional=0):\n",
    "    def _show_on_single_plot(ax):\n",
    "        if h_v == \"v\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() / 2\n",
    "                _y = p.get_y() + p.get_height()\n",
    "                value = int(p.get_height())\n",
    "                ax.text(_x, _y, value, ha=\"center\")\n",
    "        elif h_v == \"h\":\n",
    "            for p in ax.patches:\n",
    "                _x = p.get_x() + p.get_width() + float(space)\n",
    "                _y = p.get_y() + p.get_height() - 0.2\n",
    "                value = int(p.get_width())\n",
    "                ax.text(_x, _y, value, ha=\"left\")\n",
    "                current_xlim = ax.get_xlim()\n",
    "                current_xlim = (current_xlim[0], current_xlim[1] + xlim_additional)\n",
    "                ax.set_xlim(current_xlim)\n",
    "\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        for idx, ax in np.ndenumerate(axs):\n",
    "            _show_on_single_plot(ax)\n",
    "    else:\n",
    "        _show_on_single_plot(axs)\n",
    "\n",
    "\n",
    "def full_runtime_stats(pg, clipping=True, metric=\"times_elapsed\", consider_last_n=21):\n",
    "    if clipping:\n",
    "        table_title_prefix = \"\"\n",
    "        param_grid = _filter_out_param(pg, \"uncertainty_clipping\", [0.95, 0.9, 0.7])\n",
    "    else:\n",
    "        table_title_prefix = \"clipped\"\n",
    "        param_grid = _filter_out_param(pg, \"\", [])\n",
    "\n",
    "    for exp_name in param_grid[\"exp_name\"]:\n",
    "        for transformer_model_name in param_grid[\"transformer_model_name\"]:\n",
    "            for initially_labeled_samples in param_grid[\"initially_labeled_samples\"]:\n",
    "                for batch_size in param_grid[\"batch_size\"]:\n",
    "                    for num_iteration in param_grid[\"num_iterations\"]:\n",
    "                        datasets = param_grid[\"dataset\"]\n",
    "                        table_file = Path(\n",
    "                            f\"final/merge_datasets_{metric}_{table_title_prefix}_{exp_name}_{transformer_model_name}_{consider_last_n}_{initially_labeled_samples}_{batch_size}_{num_iteration}.pdf\"\n",
    "                        )\n",
    "                        print(table_file)\n",
    "\n",
    "                        groups = []\n",
    "                        for dataset in datasets:\n",
    "                            grouped_data = _load_grouped_data(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid,\n",
    "                                num_iteration,\n",
    "                                metric,\n",
    "                            )\n",
    "                            groups.append((dataset, grouped_data))\n",
    "\n",
    "                        if len(grouped_data) == 0:\n",
    "                            return\n",
    "\n",
    "                        # sum up elapsed times\n",
    "                        df_data = defaultdict(lambda: 0)\n",
    "                        for (dataset, group) in groups:\n",
    "                            for k, v in group.items():\n",
    "                                for value in v:\n",
    "                                    df_data[k] += sum(value)\n",
    "\n",
    "                        df_data2 = []\n",
    "                        for k, v in df_data.items():\n",
    "                            if k in [\n",
    "                                \"Rand (softmax) True/1.0\",\n",
    "                                \"passive (softmax) True/1.0\",\n",
    "                            ]:\n",
    "                                continue\n",
    "                            df_data2.append([_rename_strat(k, clipping=False), v / 60])\n",
    "\n",
    "                        data_df = pd.DataFrame(df_data2, columns=[\"Strategy\", metric])\n",
    "\n",
    "                        data_df.sort_values(by=metric, inplace=True)\n",
    "\n",
    "                        fig = plt.figure(\n",
    "                            figsize=set_matplotlib_size(width, fraction=0.5)\n",
    "                        )\n",
    "                        ax = sns.barplot(data=data_df, y=\"Strategy\", x=metric)\n",
    "\n",
    "                        show_values_on_bars(ax, \"h\", xlim_additional=30)\n",
    "\n",
    "                        plt.xlabel(\"\")\n",
    "                        plt.ylabel(\"\")\n",
    "\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(\n",
    "                            table_file, dpi=300, bbox_inches=\"tight\", pad_inches=0\n",
    "                        )\n",
    "                        # plt.show()\n",
    "                        plt.clf()\n",
    "                        plt.close(\"all\")\n",
    "\n",
    "\n",
    "def full_passive_comparison(\n",
    "    pg, clipping=True, metric=\"times_elapsed\", consider_last_n=21\n",
    "):\n",
    "    # get list of outliers based on passive\n",
    "    # test, which strategy has queried the most outliers\n",
    "    # does it change when using uncertainty clipping?!\n",
    "\n",
    "    if clipping:\n",
    "        table_title_prefix = \"\"\n",
    "        param_grid = _filter_out_param(pg, \"uncertainty_clipping\", [0.95, 0.9, 0.7])\n",
    "    else:\n",
    "        table_title_prefix = \"clipped\"\n",
    "        param_grid = _filter_out_param(pg, \"\", [])\n",
    "\n",
    "    for exp_name in param_grid[\"exp_name\"]:\n",
    "        for transformer_model_name in param_grid[\"transformer_model_name\"]:\n",
    "            for initially_labeled_samples in param_grid[\"initially_labeled_samples\"]:\n",
    "                for batch_size in param_grid[\"batch_size\"]:\n",
    "                    for num_iteration in param_grid[\"num_iterations\"]:\n",
    "                        datasets = param_grid[\"dataset\"]\n",
    "                        table_file = Path(\n",
    "                            f\"final/merge_datasets_{metric}_{table_title_prefix}_{exp_name}_{transformer_model_name}_{consider_last_n}_{initially_labeled_samples}_{batch_size}_{num_iteration}.pdf\"\n",
    "                        )\n",
    "                        print(table_file)\n",
    "\n",
    "                        groups = []\n",
    "                        for dataset in datasets:\n",
    "                            grouped_data = _load_grouped_data(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid,\n",
    "                                num_iteration,\n",
    "                                metric,\n",
    "                            )\n",
    "                            groups.append((dataset, grouped_data))\n",
    "\n",
    "                        if len(grouped_data) == 0:\n",
    "                            return\n",
    "\n",
    "                        # sum up elapsed times\n",
    "                        df_data = defaultdict(lambda: 0)\n",
    "                        for (dataset, group) in groups:\n",
    "                            for k, v in group.items():\n",
    "                                for value in v:\n",
    "                                    df_data[k] += sum(value)\n",
    "\n",
    "                        df_data2 = []\n",
    "                        for k, v in df_data.items():\n",
    "                            df_data2.append(\n",
    "                                [_rename_strat(k, clipping=clipping), v / 60]\n",
    "                            )\n",
    "\n",
    "                        data_df = pd.DataFrame(df_data2, columns=[\"Strategy\", metric])\n",
    "\n",
    "                        data_df.sort_values(by=metric, inplace=True)\n",
    "\n",
    "                        fig = plt.figure(\n",
    "                            figsize=set_matplotlib_size(width, fraction=0.5)\n",
    "                        )\n",
    "                        ax = sns.barplot(data=data_df, y=\"Strategy\", x=metric)\n",
    "\n",
    "                        show_values_on_bars(ax, \"h\", xlim_additional=3)\n",
    "\n",
    "                        plt.xlabel(\"\")\n",
    "                        plt.ylabel(\"\")\n",
    "\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(\n",
    "                            table_file, dpi=300, bbox_inches=\"tight\", pad_inches=0\n",
    "                        )\n",
    "                        # plt.show()\n",
    "                        plt.clf()\n",
    "                        plt.close(\"all\")\n",
    "\n",
    "\n",
    "def _plot_class_heatmap(data, ax, title, v_min, v_max):\n",
    "    print(np.min(data.to_numpy()))\n",
    "    print(np.max(data.to_numpy()))\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        data,\n",
    "        annot=True,\n",
    "        # cmap=sns.color_palette(\"husl\", as_cmap=True),\n",
    "        center=0,\n",
    "        # square=True,\n",
    "        fmt=\".1f\",\n",
    "        ax=ax,\n",
    "        vmin=v_min,\n",
    "        vmax=v_max,\n",
    "    )\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.yaxis.set_major_formatter(PercentFormatter(100, 0))\n",
    "\n",
    "    # ax.set_title(f\"{title[0]}-{title[1]}\")\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def full_class_distribution(\n",
    "    pg,\n",
    "    datasets_to_consider=[\"trec6\", \"ag_news\"],\n",
    "    clippings=[1.0, 0.95],\n",
    "    transformer_model_name=\"bert-base-uncased\",\n",
    "):\n",
    "    # train/test class distribution\n",
    "    # class distribution per strategy\n",
    "    pg[\"dataset\"] = datasets_to_consider\n",
    "\n",
    "    def _count_unique_percentages(Ys):\n",
    "        uniques = np.unique(Ys, return_counts=True)[1]\n",
    "        return [counts / np.sum(uniques) for counts in uniques]\n",
    "\n",
    "    results = []\n",
    "    datasets = datasets_to_consider\n",
    "    for clipping in clippings:\n",
    "        param_grid = copy.deepcopy(pg)\n",
    "        param_grid[\"uncertainty_clipping\"] = [clipping]\n",
    "        param_grid[\"transformer_model_name\"] = [transformer_model_name]\n",
    "\n",
    "        for exp_name in param_grid[\"exp_name\"]:\n",
    "            for transformer_model_name in param_grid[\"transformer_model_name\"]:\n",
    "                for initially_labeled_samples in param_grid[\n",
    "                    \"initially_labeled_samples\"\n",
    "                ]:\n",
    "                    for batch_size in param_grid[\"batch_size\"]:\n",
    "                        for num_iteration in param_grid[\"num_iterations\"]:\n",
    "                            dataset_counts_test = {}\n",
    "                            dataset_counts_train = {}\n",
    "                            Y_trains = {}\n",
    "                            Y_tests = {}\n",
    "\n",
    "                            for dataset in datasets:\n",
    "                                # print(dataset)\n",
    "                                train, test, _ = load_my_dataset(\n",
    "                                    dataset, \"bert-base-uncased\", tokenization=False\n",
    "                                )\n",
    "                                train_Y = train[\"label\"]\n",
    "                                test_Y = test[\"label\"]\n",
    "\n",
    "                                train_Y_uniques = _count_unique_percentages(train_Y)\n",
    "                                test_Y_uniques = _count_unique_percentages(test_Y)\n",
    "\n",
    "                                dataset_counts_test[dataset] = test_Y_uniques\n",
    "                                dataset_counts_train[dataset] = train_Y_uniques\n",
    "\n",
    "                                Y_trains[dataset] = train_Y\n",
    "                                Y_tests[dataset] = test_Y\n",
    "\n",
    "                            groups = {}\n",
    "                            for dataset in datasets:\n",
    "                                grouped_data = _load_grouped_data(\n",
    "                                    exp_name,\n",
    "                                    transformer_model_name,\n",
    "                                    dataset,\n",
    "                                    initially_labeled_samples,\n",
    "                                    batch_size,\n",
    "                                    param_grid,\n",
    "                                    num_iteration,\n",
    "                                    metric=\"queried_indices\",\n",
    "                                )\n",
    "                                grouped_data = {\n",
    "                                    _rename_strat(k, clipping=False): v\n",
    "                                    for k, v in grouped_data.items()\n",
    "                                }\n",
    "                                groups[dataset] = grouped_data\n",
    "\n",
    "                            if len(grouped_data) == 0:\n",
    "                                return\n",
    "\n",
    "                            for dataset in datasets:\n",
    "                                queried_percentages = {}\n",
    "                                for strategy in groups[dataset].keys():\n",
    "                                    queried_percentages[strategy] = []\n",
    "                                    for random_seed in range(\n",
    "                                        0, np.shape(groups[dataset][strategy])[0]\n",
    "                                    ):\n",
    "                                        queriend_indices = np.array(\n",
    "                                            groups[dataset][strategy][random_seed]\n",
    "                                        ).flatten()\n",
    "\n",
    "                                        queried_Ys = np.array(Y_trains[dataset])[\n",
    "                                            queriend_indices\n",
    "                                        ]\n",
    "\n",
    "                                        queried_percentages[strategy] = [\n",
    "                                            *queried_percentages[strategy],\n",
    "                                            *queried_Ys,\n",
    "                                        ]\n",
    "\n",
    "                                    queried_percentages[\n",
    "                                        strategy\n",
    "                                    ] = _count_unique_percentages(\n",
    "                                        queried_percentages[strategy]\n",
    "                                    )\n",
    "\n",
    "                                df = pd.DataFrame(queried_percentages)\n",
    "                                df[\"Test\"] = dataset_counts_test[dataset]\n",
    "                                df[\"Train\"] = dataset_counts_train[dataset]\n",
    "\n",
    "                                # normalize data using true distribution in train test\n",
    "                                df = df.apply(lambda col: col - df[\"Train\"])\n",
    "                                df = df.apply(lambda x: x * 100)\n",
    "\n",
    "                                del df[\"Train\"]\n",
    "                                if \"Pass\" in df.columns:\n",
    "                                    del df[\"Pass\"]\n",
    "\n",
    "                                df = df.T\n",
    "                                df = df.rename(columns=lambda c: chr(ord(\"A\") + c))\n",
    "\n",
    "                                results.append((dataset, clipping, df))\n",
    "\n",
    "    # create 4 supblots\n",
    "    fig, axs = plt.subplots(\n",
    "        2,\n",
    "        2,\n",
    "        figsize=set_matplotlib_size(width, fraction=1.0),\n",
    "    )\n",
    "\n",
    "    min_ag_news = -20\n",
    "    max_ag_news = 20\n",
    "\n",
    "    min_trec = -15\n",
    "    max_trec = 15\n",
    "\n",
    "    ax00 = _plot_class_heatmap(\n",
    "        results[0][2],\n",
    "        ax=axs[0, 0],\n",
    "        title=results[0],\n",
    "        v_min=min_trec,\n",
    "        v_max=max_trec,\n",
    "    )\n",
    "\n",
    "    ax01 = _plot_class_heatmap(\n",
    "        results[1][2],\n",
    "        ax=axs[0, 1],\n",
    "        title=results[1],\n",
    "        v_min=min_ag_news,\n",
    "        v_max=max_ag_news,\n",
    "    )\n",
    "    ax10 = _plot_class_heatmap(\n",
    "        results[2][2],\n",
    "        ax=axs[1, 0],\n",
    "        title=results[2],\n",
    "        v_min=min_trec,\n",
    "        v_max=max_trec,\n",
    "    )\n",
    "    ax11 = _plot_class_heatmap(\n",
    "        results[3][2],\n",
    "        ax=axs[1, 1],\n",
    "        title=results[3],\n",
    "        v_min=min_ag_news,\n",
    "        v_max=max_ag_news,\n",
    "    )\n",
    "\n",
    "    for axa in [ax00, ax01, ax10, ax11]:\n",
    "        axa.set_xlabel(\"\")\n",
    "        axa.set_ylabel(\"\")\n",
    "        axa.tick_params(axis=\"x\", bottom=False)\n",
    "\n",
    "    ax00.set_ylabel(\"Clipping 100\\%\")\n",
    "    ax10.set_ylabel(\"Clipping 95\\%\")\n",
    "\n",
    "    # remove unecessary yaxis\n",
    "    ax01.yaxis.set_visible(False)\n",
    "    ax11.yaxis.set_visible(False)\n",
    "    ax00.xaxis.set_ticks([])\n",
    "    ax01.xaxis.set_ticks([])\n",
    "\n",
    "    ax00.set_xlabel(\"TREC6\")\n",
    "    ax00.xaxis.set_label_position(\"top\")\n",
    "    ax01.set_xlabel(\"AG_NEWS\")\n",
    "    ax01.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # remove colorbars\n",
    "    \"\"\"for axa in [ax00, ax01, ax10]:\n",
    "        cbar = axa.collections[0].colorbar\n",
    "        cbar.remove()\n",
    "\n",
    "    cbar11 = ax11.collections[0].colorbar\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.11, right=0.94, top=0.95)\n",
    "    cax = plt.axes([0.95, 0, 0.02, 1.0])\n",
    "    cbar = fig.colorbar(\n",
    "        ax11.collections[0],\n",
    "        cax=cax,\n",
    "    )\n",
    "    cbar.ax.yaxis.set_major_formatter(PercentFormatter(100, 0))\n",
    "\n",
    "    cbar11.remove()\n",
    "    \"\"\"\n",
    "    table_file = Path(f\"final/class_distributions.pdf\")\n",
    "    print(table_file)\n",
    "    plt.savefig(table_file, dpi=300, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.clf()\n",
    "    plt.close(\"all\")\n",
    "\n",
    "\n",
    "def _flatten(list_to_flatten):\n",
    "    return [num for elem in list_to_flatten for num in elem]\n",
    "\n",
    "\n",
    "def _vector_indice_heatmap(data, ax, title, vmin, vmax, other_data=None):\n",
    "    results = []\n",
    "    for (a, b) in itertools.product(data.keys(), repeat=2):\n",
    "        outliers_per_random_seed_a = set(_flatten(data[a]))\n",
    "        outliers_per_random_seed_b = set(_flatten(data[b]))\n",
    "\n",
    "        results.append(\n",
    "            (\n",
    "                a,\n",
    "                b,\n",
    "                len(outliers_per_random_seed_a.intersection(outliers_per_random_seed_b))\n",
    "                / len(outliers_per_random_seed_a.union(outliers_per_random_seed_b))\n",
    "                * 100,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if other_data:\n",
    "        other_results = []\n",
    "        for (a, b) in itertools.product(other_data.keys(), repeat=2):\n",
    "            outliers_per_random_seed_a = set(_flatten(other_data[a]))\n",
    "            outliers_per_random_seed_b = set(_flatten(other_data[b]))\n",
    "\n",
    "            other_results.append(\n",
    "                (\n",
    "                    a,\n",
    "                    b,\n",
    "                    len(\n",
    "                        outliers_per_random_seed_a.intersection(\n",
    "                            outliers_per_random_seed_b\n",
    "                        )\n",
    "                    )\n",
    "                    / len(outliers_per_random_seed_a.union(outliers_per_random_seed_b))\n",
    "                    * 100,\n",
    "                )\n",
    "            )\n",
    "        new_df = pd.DataFrame(results, columns=[\"a\", \"b\", \"agreement\"])\n",
    "        new_df = new_df.pivot(\"a\", \"b\", \"agreement\")\n",
    "        original_df = pd.DataFrame(other_results, columns=[\"a\", \"b\", \"agreement\"])\n",
    "        original_df = original_df.pivot(\"a\", \"b\", \"agreement\")\n",
    "\n",
    "        annotation_dataframe = original_df - new_df\n",
    "\n",
    "        annotation = annotation_dataframe\n",
    "    else:\n",
    "        annotation = True\n",
    "\n",
    "    result_df = pd.DataFrame(results, columns=[\"a\", \"b\", \"agreement\"])\n",
    "\n",
    "    print(\"smallest: \", result_df[\"agreement\"].min())\n",
    "    print(\"second largest: \", np.unique(result_df[\"agreement\"].to_numpy())[-2])\n",
    "\n",
    "    result_df = result_df.pivot(\"a\", \"b\", \"agreement\")\n",
    "    mask = np.zeros_like(result_df.to_numpy())\n",
    "    mask[np.diag_indices_from(mask)] = True\n",
    "\n",
    "    ax = sns.heatmap(\n",
    "        result_df,\n",
    "        annot=annotation,\n",
    "        mask=mask,\n",
    "        cmap=sns.color_palette(\"husl\", as_cmap=True),\n",
    "        # square=True,\n",
    "        fmt=\".1f\",\n",
    "        ax=ax,\n",
    "        vmin=vmin,\n",
    "        vmax=vmax,\n",
    "    )\n",
    "    # ax.set_title(f\"{title[0]}-{title[1]}\")\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def full_outlier_comparison(\n",
    "    pg,\n",
    "):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for clipping in [1.0, 0.95]:\n",
    "        param_grid = copy.deepcopy(pg)\n",
    "        param_grid[\"uncertainty_clipping\"] = [clipping]\n",
    "        for transformer_model_name in param_grid[\"transformer_model_name\"]:\n",
    "            for exp_name in param_grid[\"exp_name\"]:\n",
    "                for initially_labeled_samples in param_grid[\n",
    "                    \"initially_labeled_samples\"\n",
    "                ]:\n",
    "                    for batch_size in param_grid[\"batch_size\"]:\n",
    "                        for num_iteration in param_grid[\"num_iterations\"]:\n",
    "                            datasets = param_grid[\"dataset\"]\n",
    "\n",
    "                            groups = {}\n",
    "                            for dataset in datasets:\n",
    "                                grouped_data = _load_grouped_data(\n",
    "                                    exp_name,\n",
    "                                    transformer_model_name,\n",
    "                                    dataset,\n",
    "                                    initially_labeled_samples,\n",
    "                                    batch_size,\n",
    "                                    param_grid,\n",
    "                                    num_iteration,\n",
    "                                    metric=\"queried_indices\",\n",
    "                                    ignore_clipping_for_random_and_passive=False,\n",
    "                                )\n",
    "                                grouped_data = {\n",
    "                                    _rename_strat(k, clipping=False): v\n",
    "                                    for k, v in grouped_data.items()\n",
    "                                }\n",
    "                                groups[dataset] = grouped_data\n",
    "\n",
    "                            if len(grouped_data) == 0:\n",
    "                                return\n",
    "\n",
    "                            outliers = {}\n",
    "                            queried_indices = {}\n",
    "\n",
    "                            for dataset in datasets:\n",
    "                                outliers[dataset] = []\n",
    "                                queried_indices[dataset] = {}\n",
    "\n",
    "                                for strategy in groups[dataset].keys():\n",
    "                                    if strategy == \"Passive\":\n",
    "                                        queried_indices[dataset][\"Outlier\"] = []\n",
    "                                        for random_seed in param_grid[\"random_seed\"]:\n",
    "                                            passive_path = _convert_config_to_path(\n",
    "                                                {\n",
    "                                                    \"uncertainty_method\": \"softmax\",\n",
    "                                                    \"query_strategy\": \"passive\",\n",
    "                                                    \"exp_name\": exp_name,\n",
    "                                                    \"transformer_model_name\": transformer_model_name,\n",
    "                                                    \"dataset\": dataset,\n",
    "                                                    \"initially_labeled_samples\": initially_labeled_samples,\n",
    "                                                    \"random_seed\": random_seed,\n",
    "                                                    \"batch_size\": batch_size,\n",
    "                                                    \"num_iterations\": num_iteration,\n",
    "                                                    \"uncertainty_clipping\": \"1.0\",\n",
    "                                                    \"lower_is_better\": True,\n",
    "                                                }\n",
    "                                            )\n",
    "                                            if passive_path.exists():\n",
    "                                                metrics = np.load(\n",
    "                                                    passive_path / \"metrics.npz\",\n",
    "                                                    allow_pickle=True,\n",
    "                                                )\n",
    "                                                outliers[dataset].append(\n",
    "                                                    metrics[\"passive_outlier\"].tolist()[\n",
    "                                                        0\n",
    "                                                    ][0]\n",
    "                                                )\n",
    "\n",
    "                                                queried_indices[dataset][\n",
    "                                                    \"Outlier\"\n",
    "                                                ].append(\n",
    "                                                    [\n",
    "                                                        int(q)\n",
    "                                                        for q in metrics[\n",
    "                                                            \"passive_outlier\"\n",
    "                                                        ][0][0].tolist()\n",
    "                                                    ]\n",
    "                                                )\n",
    "                                    else:\n",
    "                                        queried_indices[dataset][strategy] = []\n",
    "                                        for random_seed in range(\n",
    "                                            0, np.shape(groups[dataset][strategy])[0]\n",
    "                                        ):\n",
    "                                            queried_indices[dataset][strategy].append(\n",
    "                                                np.array(\n",
    "                                                    groups[dataset][strategy][\n",
    "                                                        random_seed\n",
    "                                                    ]\n",
    "                                                )\n",
    "                                                .flatten()\n",
    "                                                .tolist()\n",
    "                                            )\n",
    "\n",
    "                            merged_strats = {}\n",
    "\n",
    "                            for dataset in queried_indices.keys():\n",
    "                                for strat in queried_indices[dataset].keys():\n",
    "                                    if not strat in merged_strats.keys():\n",
    "                                        merged_strats[strat] = []\n",
    "                                    merged_strats[strat].append(\n",
    "                                        _flatten(queried_indices[dataset][strat])\n",
    "                                    )\n",
    "                            results.append(\n",
    "                                (clipping, transformer_model_name, merged_strats)\n",
    "                            )\n",
    "\n",
    "    # create 4 supblots\n",
    "    fig, axs = plt.subplots(\n",
    "        2,\n",
    "        2,\n",
    "        figsize=set_matplotlib_size(width, fraction=1.0),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "\n",
    "    v_min = 11\n",
    "    v_max = 70\n",
    "\n",
    "    ax00 = _vector_indice_heatmap(\n",
    "        results[0][2], ax=axs[0, 0], title=results[0], vmin=v_min, vmax=v_max\n",
    "    )\n",
    "    ax01 = _vector_indice_heatmap(\n",
    "        results[2][2],\n",
    "        ax=axs[0, 1],\n",
    "        title=results[2],\n",
    "        vmin=v_min,\n",
    "        vmax=v_max,\n",
    "        other_data=results[0][2],\n",
    "    )\n",
    "    ax10 = _vector_indice_heatmap(\n",
    "        results[1][2], ax=axs[1, 0], title=results[1], vmin=v_min, vmax=v_max\n",
    "    )\n",
    "    ax11 = _vector_indice_heatmap(\n",
    "        results[3][2],\n",
    "        ax=axs[1, 1],\n",
    "        title=results[3],\n",
    "        vmin=v_min,\n",
    "        vmax=v_max,\n",
    "        other_data=results[0][2],\n",
    "    )\n",
    "\n",
    "    \"\"\"cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.yaxis.set_major_formatter(PercentFormatter(100, 0))                      \n",
    "    \"\"\"\n",
    "    for axa in [ax00, ax01, ax10, ax11]:\n",
    "        axa.set_xlabel(\"\")\n",
    "        axa.set_ylabel(\"\")\n",
    "        axa.tick_params(axis=\"x\", bottom=False)\n",
    "\n",
    "    from matplotlib.ticker import LogFormatter\n",
    "\n",
    "    ax00.set_ylabel(\"BERT\")\n",
    "    ax10.set_ylabel(\"RoBERTa\")\n",
    "\n",
    "    ax00.set_xlabel(\"Clipping: 100\\%\")\n",
    "    ax00.xaxis.set_label_position(\"top\")\n",
    "    ax01.set_xlabel(\"Clipping: 95\\%\")\n",
    "    ax01.xaxis.set_label_position(\"top\")\n",
    "\n",
    "    # remove colorbars\n",
    "    for axa in [ax00, ax01, ax10]:\n",
    "        cbar = axa.collections[0].colorbar\n",
    "        cbar.remove()\n",
    "\n",
    "    cbar11 = ax11.collections[0].colorbar\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplots_adjust(bottom=0.11, right=0.94, top=0.95)\n",
    "    cax = plt.axes([0.95, 0, 0.02, 1.0])\n",
    "    cbar = fig.colorbar(\n",
    "        ax11.collections[0],\n",
    "        cax=cax,\n",
    "    )\n",
    "    cbar.ax.yaxis.set_major_formatter(PercentFormatter(100, 0))\n",
    "\n",
    "    cbar11.remove()\n",
    "\n",
    "    table_file = Path(f\"final/vector_indices_all_datasets.pdf\")\n",
    "    print(table_file)\n",
    "    plt.savefig(table_file, dpi=300, bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.clf()\n",
    "    plt.close(\"all\")\n",
    "\n",
    "\n",
    "def full_uncertainty_plots(\n",
    "    param_grid,\n",
    "    # metric=\"confidence_scores\",\n",
    "    metric=\"y_proba_test_active\",\n",
    "    datasets=[\"trec6\", \"ag_news\"],\n",
    "    strategies=[\n",
    "        \"Rand (softmax) True/1.0\",\n",
    "        \"passive (softmax) True/1.0\",\n",
    "        \"trustscore (softmax) True/1.0\",\n",
    "        \"LC (label_smoothing) True/1.0\",\n",
    "    ],\n",
    "    transformer_model_name=\"bert-base-uncased\",\n",
    "):\n",
    "    for dataset in datasets:\n",
    "        grouped_data = _load_grouped_data(\n",
    "            exp_name=param_grid[\"exp_name\"][0],\n",
    "            transformer_model_name=transformer_model_name,\n",
    "            dataset=dataset,\n",
    "            initially_labeled_samples=param_grid[\"initially_labeled_samples\"][0],\n",
    "            batch_size=param_grid[\"batch_size\"][0],\n",
    "            param_grid=param_grid,\n",
    "            num_iterations=param_grid[\"num_iterations\"][0],\n",
    "            metric=metric,\n",
    "        )\n",
    "        if len(grouped_data) == 0:\n",
    "            return\n",
    "\n",
    "        df_data = []\n",
    "        for k, v in grouped_data.items():\n",
    "            if k not in strategies:\n",
    "                continue\n",
    "\n",
    "            for random_seed in v:\n",
    "                # print(random_seed)\n",
    "                for i, iteration in enumerate(random_seed):\n",
    "                    for v in iteration:\n",
    "                        if metric != \"confidence_scores\":\n",
    "                            v = np.max(v)\n",
    "                        if v < 0:\n",
    "                            v = v * (-1)\n",
    "                        df_data.append((k, v, i))\n",
    "        df = pd.DataFrame(data=df_data, columns=[\"Strategy\", metric, \"iteration\"])\n",
    "\n",
    "        for strat in df[\"Strategy\"].unique():\n",
    "            mv = df.loc[df[\"Strategy\"] == strat][metric].astype(np.float16)\n",
    "            if np.nanmax(mv) == np.inf:\n",
    "                max_value = np.iinfo(np.int16).max\n",
    "            else:\n",
    "                max_value = np.nanmax(mv)\n",
    "            if np.nanmin(mv) == 0 and max_value == 0:\n",
    "                continue\n",
    "            counts, bins = np.histogram(mv, bins=70, range=(np.nanmin(mv), max_value))\n",
    "\n",
    "            fig = plt.figure(figsize=set_matplotlib_size(width, fraction=0.33))\n",
    "            plt.hist(\n",
    "                bins[:-1],\n",
    "                weights=counts,\n",
    "                bins=bins,\n",
    "            )\n",
    "            # plt.title(f\"{strat}\")\n",
    "            plt.title(\"\")\n",
    "            plot_path = Path(f\"./plots/{metric}_{transformer_model_name}_{dataset}\")\n",
    "            plot_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "            plt.savefig(\n",
    "                plot_path / f\"{strat.replace('/', '-')}.jpg\",\n",
    "                bbox_inches=\"tight\",\n",
    "                pad_inches=0,\n",
    "            )\n",
    "            plt.savefig(\n",
    "                plot_path / f\"{strat.replace('/', '-')}.pdf\",\n",
    "                dpi=300,\n",
    "                bbox_inches=\"tight\",\n",
    "                pad_inches=0,\n",
    "            )\n",
    "            plt.clf()\n",
    "            plt.close(\"all\")\n",
    "\n",
    "            for iteration in df[\"iteration\"].unique():\n",
    "                mv = df.loc[(df[\"Strategy\"] == strat) & (df[\"iteration\"] == iteration)][\n",
    "                    metric\n",
    "                ].astype(np.float16)\n",
    "                if np.nanmax(mv) == np.inf:\n",
    "                    max_value = np.iinfo(np.int16).max\n",
    "                else:\n",
    "                    max_value = np.nanmax(mv)\n",
    "                if np.nanmin(mv) == 0 and max_value == 0:\n",
    "                    continue\n",
    "                counts, bins = np.histogram(\n",
    "                    mv, bins=70, range=(np.nanmin(mv), max_value)\n",
    "                )\n",
    "\n",
    "                fig = plt.figure(figsize=set_matplotlib_size(width, fraction=0.33))\n",
    "                plt.hist(\n",
    "                    bins[:-1],\n",
    "                    weights=counts,\n",
    "                    bins=bins,\n",
    "                )\n",
    "\n",
    "                #  plt.title(f\"{strat}: {iteration}\")\n",
    "                plt.title(\"\")\n",
    "                plot_path = Path(\n",
    "                    f\"./plots/{metric}_{transformer_model_name}_{dataset}/{strat.replace('/', '-')}/\"\n",
    "                )\n",
    "                plot_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "                plt.savefig(\n",
    "                    plot_path / f\"{iteration}.jpg\", bbox_inches=\"tight\", pad_inches=0\n",
    "                )\n",
    "                plt.savefig(\n",
    "                    plot_path / f\"{iteration}.pdf\",\n",
    "                    dpi=300,\n",
    "                    bbox_inches=\"tight\",\n",
    "                    pad_inches=0,\n",
    "                )\n",
    "                print(plot_path / f\"{iteration}.jpg\")\n",
    "                plt.clf()\n",
    "                plt.close(\"all\")\n",
    "\n",
    "\n",
    "def _generate_al_strat_abbreviations_table(pg):\n",
    "    param_grid = copy.deepcopy(pg)\n",
    "    param_grid[\"uncertainty_clipping\"] = [1.0]\n",
    "\n",
    "    data = _load_grouped_data(\n",
    "        exp_name=param_grid[\"exp_name\"][0],\n",
    "        transformer_model_name=\"bert-base-uncased\",\n",
    "        dataset=\"trec6\",\n",
    "        initially_labeled_samples=param_grid[\"initially_labeled_samples\"][0],\n",
    "        batch_size=param_grid[\"batch_size\"][0],\n",
    "        param_grid=param_grid,\n",
    "        num_iterations=param_grid[\"num_iterations\"][0],\n",
    "        metric=\"test_acc\",\n",
    "    )\n",
    "\n",
    "    strats = []\n",
    "    for key, _ in data.items():\n",
    "        strats.append((key, _rename_strat(key)))\n",
    "    df = pd.DataFrame(strats, columns=[\"AL strategy\", \"Abbreviation\"])\n",
    "    print(tabulate(df, headers=\"keys\", showindex=False, tablefmt=\"latex_booktabs\"))\n",
    "\n",
    "\n",
    "#full_param_grid[\"dataset\"].remove(\"cola\")\n",
    "#full_param_grid[\"dataset\"].remove(\"sst2\")\n",
    "\n",
    "\n",
    "# _generate_al_strat_abbreviations_table(full_param_grid)\n",
    "# full_uncertainty_plots(full_param_grid, metric=\"confidence_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final/violinplots_test_acc_baseline_bert-base-uncased_5_25_25_20.pdf\n",
      "['KLD', 'VE', 'Ent', 'IS', 'TeSc', 'Rand', 'LC', 'MM', 'Evi', 'LS', 'MoCa']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 496.063x183.95 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def full_violinplot(pg, metric=\"test_acc\", consider_last_n=21):\n",
    "    param_grid = _filter_out_param(pg, \"uncertainty_clipping\", [0.9, 0.7])\n",
    "\n",
    "    for exp_name in param_grid[\"exp_name\"]:\n",
    "        for transformer_model_name in param_grid[\"transformer_model_name\"]:\n",
    "            for initially_labeled_samples in param_grid[\"initially_labeled_samples\"]:\n",
    "                for batch_size in param_grid[\"batch_size\"]:\n",
    "                    for num_iteration in param_grid[\"num_iterations\"]:\n",
    "                        datasets = param_grid[\"dataset\"]\n",
    "\n",
    "                        plot_file = Path(\n",
    "                            f\"final/violinplots_{metric}_{exp_name}_{transformer_model_name}_{consider_last_n}_{initially_labeled_samples}_{batch_size}_{num_iteration}.pdf\"\n",
    "                        )\n",
    "                        print(plot_file)\n",
    "\n",
    "                        groups = []\n",
    "                        for dataset in datasets:\n",
    "                            grouped_data = _load_grouped_data(\n",
    "                                exp_name,\n",
    "                                transformer_model_name,\n",
    "                                dataset,\n",
    "                                initially_labeled_samples,\n",
    "                                batch_size,\n",
    "                                param_grid,\n",
    "                                num_iteration,\n",
    "                                metric,\n",
    "                            )\n",
    "                            groups.append((dataset, grouped_data))\n",
    "\n",
    "                        table_data = []\n",
    "                        table_data2 = []\n",
    "                        stick_data = {}\n",
    "\n",
    "                        for (dataset, group) in groups:\n",
    "                            dataset2 = _rename_dataset_name(dataset)\n",
    "                            stick_data[dataset2] = []\n",
    "\n",
    "                            for k, v in group.items():\n",
    "                                if k[-3:] == \"1.0\":\n",
    "                                    clipping = \"Original\"\n",
    "                                elif k[-3:] == \"0.9\":\n",
    "                                    clipping = \"90%\"\n",
    "                                elif k[-4:] == \"0.95\":\n",
    "                                    clipping = \"95\\%\"\n",
    "                                else:\n",
    "                                    print(\"help\" * 1000)\n",
    "\n",
    "                                k = _rename_strat(k, clipping=False)\n",
    "                                if k == \"Passive\":\n",
    "                                    continue\n",
    "                                v = [x[-consider_last_n:] for x in v]\n",
    "                                v = np.mean(v, axis=1)\n",
    "\n",
    "                                for formatted_value in v:\n",
    "                                    formatted_value *= 100\n",
    "                                    table_data.append((k, formatted_value, clipping))\n",
    "\n",
    "                                    if k == \"Rand\":\n",
    "                                        table_data.append((k, formatted_value, \"95\\%\"))\n",
    "\n",
    "                                formatted_value = np.mean(v) * 100\n",
    "                                table_data2.append((k, formatted_value, clipping))\n",
    "                                if k == \"Rand\":\n",
    "                                    table_data2.append((k, formatted_value, \"95\\%\"))\n",
    "                                stick_data[dataset2].append(formatted_value)\n",
    "\n",
    "                        df = pd.DataFrame(\n",
    "                            table_data, columns=[\"Method\", \"Acc\", \"clipping\"]\n",
    "                        )\n",
    "                        df2 = df.groupby([\"Method\", \"clipping\"]).mean().sort_values(\"Acc\")\n",
    "\n",
    "                        df3 = pd.DataFrame(\n",
    "                            table_data2, columns=[\"Method\", \"Acc\", \"clipping\"]\n",
    "                        )\n",
    "                        df4 = (\n",
    "                            df3.loc[df3[\"clipping\"] == \"95\\%\"]\n",
    "                            .groupby([\"Method\", \"clipping\"])\n",
    "                            .mean()\n",
    "                            .sort_values(\"Acc\")\n",
    "                        )\n",
    "\n",
    "                        ordering = df4.index.tolist()\n",
    "                        ordering = [o[0] for o in ordering]\n",
    "                        print(ordering)\n",
    "\n",
    "                        fig_dim = set_matplotlib_size(width, fraction=1.0)\n",
    "                        fig_dim = (fig_dim[0], fig_dim[1] * 0.6)\n",
    "                        fig = plt.figure(figsize=fig_dim)\n",
    "                        \"\"\"ax = sns.violinplot(\n",
    "                            data=df3,\n",
    "                            y=\"Acc\",\n",
    "                            x=\"Method\",\n",
    "                            order=ordering,\n",
    "                            hue=\"clipping\",\n",
    "                            split=True,\n",
    "                            inner=\"stick\",\n",
    "                        )\"\"\"\n",
    "                        ax2 = sns.violinplot(\n",
    "                            data=df,\n",
    "                            y=\"Acc\",\n",
    "                            x=\"Method\",\n",
    "                            order=ordering,\n",
    "                            hue=\"clipping\",\n",
    "                            split=True,\n",
    "                            bw_adjust=0.4,\n",
    "                            palette=[\".85\", \".4\"],\n",
    "                            fill=False,\n",
    "                            dodge=True,\n",
    "                            inner=\"box\",\n",
    "                            inner_kws={\"marker\":\"D\"}\n",
    "                            # cut=1,\n",
    "                            # ax=ax,\n",
    "                        )\n",
    "\n",
    "                        violins = [\n",
    "                            art\n",
    "                            for art in ax2.get_children()\n",
    "                            if isinstance(art, PolyCollection)\n",
    "                        ]\n",
    "\n",
    "                        for violin in violins:\n",
    "                            violin.set_alpha(0)\n",
    "\n",
    "                        plt.show()\n",
    "                        exit(-1)\n",
    "                        return\n",
    "\n",
    "                        violins = [\n",
    "                            art\n",
    "                            for art in ax.get_children()\n",
    "                            if isinstance(art, PolyCollection)\n",
    "                        ]\n",
    "\n",
    "                        for violin in violins:\n",
    "                            violin.set_alpha(0)\n",
    "\n",
    "                        ax2 = sns.violinplot(\n",
    "                            data=df,\n",
    "                            y=\"Acc\",\n",
    "                            x=\"Method\",\n",
    "                            order=ordering,\n",
    "                            hue=\"clipping\",\n",
    "                            split=True,\n",
    "                            bw_adjust=0.4,\n",
    "                            palette=[\".85\", \".4\"],\n",
    "                            # cut=1,\n",
    "                            # ax=ax,\n",
    "                        )\n",
    "                        old_handles, old_labels = ax2.get_legend_handles_labels()\n",
    "                        old_handles = old_handles[2:]\n",
    "\n",
    "                        dataset_colors = {\n",
    "                            dataset_name: sns.color_palette(\n",
    "                                palette=\"colorblind\", n_colors=len(stick_data.keys())\n",
    "                            )[ix]\n",
    "                            for ix, dataset_name in enumerate(stick_data.keys())\n",
    "                        }\n",
    "                        for k, v in dataset_colors.items():\n",
    "                            dataset_colors[k] = (*v, 0.9)\n",
    "\n",
    "                        for l in ax.lines:\n",
    "                            for dataset_name in stick_data.keys():\n",
    "                                if l.get_data()[1][0] in stick_data[dataset_name]:\n",
    "                                    l.set_color(dataset_colors[dataset_name])\n",
    "                                    l.set_linewidth(9)\n",
    "                                    l.set_linestyle(\"-\")\n",
    "\n",
    "                        dataset_legend_handles = []\n",
    "                        for dataset, dataset_color in dataset_colors.items():\n",
    "                            dataset_legend_handles.append(\n",
    "                                Line2D(\n",
    "                                    [0], [0], color=dataset_color, lw=6, label=dataset\n",
    "                                ),\n",
    "                            )\n",
    "\n",
    "                        dataset_legend_handles = [*dataset_legend_handles, *old_handles]\n",
    "\n",
    "                        ax.legend(\n",
    "                            handles=dataset_legend_handles,\n",
    "                            loc=\"lower center\",\n",
    "                            ncol=2 + len(dataset_colors.keys()),\n",
    "                        )\n",
    "\n",
    "                        \"\"\"ax.set_xticklabels(\n",
    "                            ax.get_xticklabels(),\n",
    "                            rotation=20,\n",
    "                            horizontalalignment=\"right\",\n",
    "                        )\"\"\"\n",
    "                        \n",
    "                        plt.xlabel(\"\")\n",
    "                        plt.ylabel(\"\")\n",
    "                        plt.ylim(50, 100)\n",
    "                        plt.show()\n",
    "                        #plt.tight_layout()\n",
    "                        #plt.savefig(\n",
    "                        #    plot_file, dpi=300, bbox_inches=\"tight\", pad_inches=0\n",
    "                        #)\n",
    "                        #plt.clf()\n",
    "                        #plt.close(\"all\")\n",
    "\n",
    "full_violinplot(copy.deepcopy(full_param_grid), consider_last_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "active-learning-softmax-uncertainty-clippi-y7QO68ki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
